"""
LLM Reviewer
──────────────
Uses an LLM (OpenAI / Ollama / Anthropic) to perform semantic
code analysis and generate intelligent review comments.
"""

import json
import logging
import os
from dataclasses import dataclass, field
from typing import Optional

logger = logging.getLogger(__name__)


@dataclass
class LLMReviewComment:
    """A review comment generated by the LLM."""
    file_path: str
    line_number: int
    severity: str
    problem: str
    suggestion: str
    code_example: str = ""
    confidence: float = 0.8
    reasoning: str = ""

    def to_dict(self) -> dict:
        """Serialise the comment to a plain dictionary for JSON output."""
        return {
            "file_path": self.file_path,
            "line_number": self.line_number,
            "severity": self.severity,
            "problem": self.problem,
            "suggestion": self.suggestion,
            "code_example": self.code_example,
            "confidence": self.confidence,
            "reasoning": self.reasoning,
        }


REVIEW_SYSTEM_PROMPT = """You are an expert code reviewer. Analyse the provided code diff and generate precise, actionable review comments.

For each issue found, respond with a JSON array of objects, each containing:
- "file_path": the file being reviewed
- "line_number": the specific line number with the issue (from the new file)
- "severity": one of "critical", "error", "warning", "info"
- "problem": clear description of the problem
- "suggestion": actionable fix recommendation
- "code_example": corrected code snippet (if applicable)
- "confidence": float 0.0-1.0 indicating confidence
- "reasoning": brief explanation of why this is an issue

Focus on:
1. Logic errors and bugs
2. Performance issues
3. Security vulnerabilities
4. API misuse or anti-patterns
5. Error handling gaps
6. Concurrency issues

Do NOT flag:
- Pure style issues (handled by linter)
- Minor naming preferences
- Import ordering

Respond ONLY with a valid JSON array. If no issues are found, respond with [].
"""


REFACTOR_SYSTEM_PROMPT = """You are an expert code refactoring assistant. Given source code and identified issues, produce the refactored version of the code.

Respond with a JSON object containing:
- "refactored_code": the complete refactored source code
- "changes": array of objects describing each change, with:
  - "description": what was changed
  - "line_range": [start, end] of affected lines
  - "reason": why this change improves the code

Apply these refactoring patterns:
1. Extract Method: Break complex functions into smaller ones
2. Rename Variable: Use descriptive, meaningful names
3. Simplify Conditional: Reduce nested if/else chains
4. Remove Dead Code: Eliminate unreachable or unused code

Respond ONLY with valid JSON.
"""


class LLMReviewer:
    """LLM-powered semantic code reviewer."""

    def __init__(
        self,
        provider: Optional[str] = None,
        api_key: Optional[str] = None,
        model: Optional[str] = None,
    ):
        """Initialise the reviewer with an LLM provider and model."""
        self.provider = provider or os.getenv("LLM_PROVIDER", "ollama")
        self.api_key = api_key or os.getenv("OPENAI_API_KEY", "")
        self.model = model or self._default_model()
        logger.info("LLM Reviewer initialised: provider=%s, model=%s", self.provider, self.model)

    def _default_model(self) -> str:
        """Return the default model name for the configured provider."""
        defaults = {
            "ollama": "codellama",
            "openai": "gpt-4o-mini",
            "anthropic": "claude-3-haiku-20240307",
        }
        return defaults.get(self.provider, "gpt-4o-mini")

    # ── Review Code ───────────────────────────────────────────────────────

    def review_code(
        self,
        file_path: str,
        source_code: str,
        diff_content: str,
        context: Optional[str] = None,
    ) -> list[LLMReviewComment]:
        """
        Send code + diff to LLM for semantic review.
        Returns structured review comments.
        """
        user_prompt = self._build_review_prompt(file_path, source_code, diff_content, context)

        try:
            response = self._call_llm(REVIEW_SYSTEM_PROMPT, user_prompt)
            comments = self._parse_review_response(response, file_path)
            logger.info("LLM review generated %d comments for %s.", len(comments), file_path)
            return comments
        except Exception as exc:
            logger.error("LLM review failed for %s: %s", file_path, exc)
            return []

    def generate_refactoring(
        self,
        file_path: str,
        source_code: str,
        issues: list[dict],
    ) -> Optional[dict]:
        """
        Ask LLM to generate refactored code based on identified issues.
        Returns dict with 'refactored_code' and 'changes' list.
        """
        user_prompt = self._build_refactor_prompt(file_path, source_code, issues)

        try:
            response = self._call_llm(REFACTOR_SYSTEM_PROMPT, user_prompt)
            result = json.loads(response)
            logger.info("LLM generated refactoring for %s.", file_path)
            return result
        except Exception as exc:
            logger.error("LLM refactoring failed for %s: %s", file_path, exc)
            return None

    # ── Prompt Builders ───────────────────────────────────────────────────

    def _build_review_prompt(
        self, file_path: str, source_code: str, diff_content: str, context: Optional[str]
    ) -> str:
        """Assemble the user prompt for a code review request."""
        prompt_parts = [
            f"## File: {file_path}\n",
            "### Diff:\n```diff\n" + diff_content + "\n```\n",
            "### Full File Content:\n```python\n" + source_code + "\n```\n",
        ]
        if context:
            prompt_parts.append(f"### Additional Context:\n{context}\n")
        return "\n".join(prompt_parts)

    def _build_refactor_prompt(
        self, file_path: str, source_code: str, issues: list[dict]
    ) -> str:
        """Assemble the user prompt for a refactoring request."""
        issues_text = json.dumps(issues, indent=2)
        return (
            f"## File: {file_path}\n\n"
            f"### Source Code:\n```python\n{source_code}\n```\n\n"
            f"### Identified Issues:\n```json\n{issues_text}\n```\n\n"
            "Refactor the code to address the issues above."
        )

    # ── LLM API Calls ─────────────────────────────────────────────────────

    def _call_llm(self, system_prompt: str, user_prompt: str) -> str:
        """Route to the appropriate LLM provider."""
        if self.provider == "openai":
            return self._call_openai(system_prompt, user_prompt)
        elif self.provider == "ollama":
            return self._call_ollama(system_prompt, user_prompt)
        elif self.provider == "anthropic":
            return self._call_anthropic(system_prompt, user_prompt)
        else:
            raise ValueError(f"Unknown LLM provider: {self.provider}")

    def _call_openai(self, system_prompt: str, user_prompt: str) -> str:
        """Send a chat completion request to the OpenAI API."""
        from openai import OpenAI

        client = OpenAI(api_key=self.api_key)
        response = client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.1,
            max_tokens=4096,
            response_format={"type": "json_object"},
        )
        return response.choices[0].message.content

    def _call_ollama(self, system_prompt: str, user_prompt: str) -> str:
        """Send a chat request to a local Ollama instance."""
        import requests

        url = os.getenv("OLLAMA_URL", "http://localhost:11434")
        response = requests.post(
            f"{url}/api/chat",
            json={
                "model": self.model,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                "stream": False,
                "format": "json",
            },
            timeout=120,
        )
        response.raise_for_status()
        return response.json()["message"]["content"]

    def _call_anthropic(self, system_prompt: str, user_prompt: str) -> str:
        """Send a messages request to the Anthropic API."""
        import requests

        api_key = os.getenv("ANTHROPIC_API_KEY", self.api_key)
        response = requests.post(
            "https://api.anthropic.com/v1/messages",
            headers={
                "x-api-key": api_key,
                "anthropic-version": "2023-06-01",
                "content-type": "application/json",
            },
            json={
                "model": self.model,
                "max_tokens": 4096,
                "system": system_prompt,
                "messages": [{"role": "user", "content": user_prompt}],
            },
            timeout=120,
        )
        response.raise_for_status()
        return response.json()["content"][0]["text"]

    # ── Response Parsing ──────────────────────────────────────────────────

    def _parse_review_response(
        self, response: str, file_path: str
    ) -> list[LLMReviewComment]:
        """Parse LLM JSON response into LLMReviewComment objects."""
        try:
            data = json.loads(response)
            # Handle both {"issues": [...]} and [...] formats
            if isinstance(data, dict):
                items = data.get("issues", data.get("comments", data.get("review", [])))
            elif isinstance(data, list):
                items = data
            else:
                logger.warning("Unexpected LLM response format.")
                return []

            comments = []
            for item in items:
                comments.append(LLMReviewComment(
                    file_path=item.get("file_path", file_path),
                    line_number=item.get("line_number", 0),
                    severity=item.get("severity", "info"),
                    problem=item.get("problem", ""),
                    suggestion=item.get("suggestion", ""),
                    code_example=item.get("code_example", ""),
                    confidence=float(item.get("confidence", 0.8)),
                    reasoning=item.get("reasoning", ""),
                ))
            return comments

        except (json.JSONDecodeError, KeyError, TypeError) as exc:
            logger.error("Failed to parse LLM response: %s", exc)
            return []
